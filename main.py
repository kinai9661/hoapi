from fastapi import FastAPI, HTTPException, Response
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from huggingface_hub import InferenceClient
import os
import requests
import time
import io

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="Leapcell AI Station")

# å…è¨±è·¨åŸŸ (CORS)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- è¨­å®š ---
HF_TOKEN = os.getenv("HF_TOKEN")

# æ–‡å­—æ¨¡å‹ (Chat) - ç¹¼çºŒä½¿ç”¨ InferenceClient
TEXT_MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"

# åœ–ç‰‡æ¨¡å‹ (Image) - ä½¿ç”¨ç›´æ¥ API ç¶²å€ä»¥é¿å… 402 ä»˜è²»éŒ¯èª¤
# æ¨è–¦å…è²»æ¨¡å‹:
# 1. stabilityai/stable-diffusion-3.5-large (ç•«è³ªå¥½ï¼Œé€šå¸¸å…è²»)
# 2. stabilityai/stable-diffusion-2-1 (ç©©å®š)
# 3. runwayml/stable-diffusion-v1-5 (æœ€ç©©å®šçš„å…è²»è€ç‰Œæ¨¡å‹)
IMAGE_MODEL_ID = "stabilityai/stable-diffusion-3.5-large"
IMAGE_API_URL = f"https://api-inference.huggingface.co/models/{IMAGE_MODEL_ID}"

# åˆå§‹åŒ–æ–‡å­—å®¢æˆ¶ç«¯
client = InferenceClient(token=HF_TOKEN)

@app.get("/", response_class=HTMLResponse)
def read_root():
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Leapcell AI Station</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #f0f2f5; }
            .container { background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            input, button { padding: 10px; margin: 5px 0; width: 100%; box-sizing: border-box; }
            button { background: #007bff; color: white; border: none; cursor: pointer; border-radius: 5px; }
            button:hover { background: #0056b3; }
            #result-img { max-width: 100%; margin-top: 10px; border-radius: 5px; display: none; }
            .loading { color: #666; font-style: italic; display: none; }
            .status { font-size: 0.8em; color: #888; margin-top: 5px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ¨ AI åœ–ç‰‡ç”Ÿæˆ (Direct API)</h1>
            <p class="status">Model: stabilityai/stable-diffusion-3.5-large</p>
            
            <input type="text" id="prompt" placeholder="è¼¸å…¥æç¤ºè© (ä¾‹å¦‚: Cyberpunk city, neon lights)" value="A futuristic city with flying cars, high quality, 8k">
            <button onclick="generateImage()">ç”Ÿæˆåœ–ç‰‡ (Generate)</button>
            
            <p id="loading" class="loading">æ­£åœ¨è«‹æ±‚ HF å…è²» API... è‹¥æ¨¡å‹ä¼‘çœ ä¸­å¯èƒ½éœ€è¦ 20-30 ç§’å–šé†’ã€‚</p>
            <p id="error" style="color: red; display: none;"></p>
            <img id="result-img" alt="Generated Image" />
        </div>

        <script>
            async function generateImage() {
                const prompt = document.getElementById('prompt').value;
                const img = document.getElementById('result-img');
                const loading = document.getElementById('loading');
                const error = document.getElementById('error');
                
                if(!prompt) return alert("è«‹è¼¸å…¥æç¤ºè©");

                img.style.display = 'none';
                error.style.display = 'none';
                loading.style.display = 'block';

                try {
                    const response = await fetch(`/api/generate-image?prompt=${encodeURIComponent(prompt)}`);
                    if (!response.ok) {
                        const errText = await response.text();
                        throw new Error(errText);
                    }
                    const blob = await response.blob();
                    img.src = URL.createObjectURL(blob);
                    img.style.display = 'block';
                } catch (e) {
                    error.innerText = "éŒ¯èª¤: " + e.message;
                    error.style.display = 'block';
                } finally {
                    loading.style.display = 'none';
                }
            }
        </script>
    </body>
    </html>
    """

@app.get("/api/generate-image")
async def generate_image(prompt: str):
    if not HF_TOKEN:
        raise HTTPException(status_code=500, detail="Server Error: Missing HF Token")

    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    payload = {"inputs": prompt}

    # é‡è©¦é‚è¼¯ï¼šè™•ç†æ¨¡å‹è¼‰å…¥ (503)
    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = requests.post(IMAGE_API_URL, headers=headers, json=payload)
            
            if response.status_code == 200:
                # æˆåŠŸï¼Œç›´æ¥å›å‚³åœ–ç‰‡
                return Response(content=response.content, media_type="image/png")
            
            elif response.status_code == 503:
                # æ¨¡å‹æ­£åœ¨è¼‰å…¥ä¸­ (Model Loading)
                error_data = response.json()
                estimated_time = error_data.get("estimated_time", 10)
                print(f"Model loading, waiting {estimated_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(min(estimated_time, 10)) # æœ€å¤šç­‰ 10 ç§’å†è©¦
                continue # é‡è©¦
            
            else:
                # å…¶ä»–éŒ¯èª¤ (å¦‚ 402, 400 ç­‰)
                raise HTTPException(status_code=response.status_code, detail=f"HF API Error: {response.text}")

        except requests.exceptions.RequestException as e:
            raise HTTPException(status_code=500, detail=f"Request failed: {str(e)}")

    raise HTTPException(status_code=503, detail="Model is too busy or taking too long to load. Please try again later.")

@app.post("/api/chat")
async def generate_chat(prompt: str):
    try:
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(messages=messages, model=TEXT_MODEL_ID, max_tokens=500)
        return {"result": response.choices[0].message.content}
    except Exception as e:
        raise HTTPException(status_code=503, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
